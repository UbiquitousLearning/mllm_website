---
title: Introduction
description: Introduction to mllm
prev: false
sidebar:
    order: 1
---

mllm is a fast and lightweight multimodal LLM inference engine for mobile and edge devices.

* Plain C/C++ implementation without dependencies
* Optimized for multimodal LLMs like fuyu-8B
* Supported: ARM NEON and x86 AVX2
* 4-bit and 6-bit integer quantization



## Give it a try
mllm provides a series of [example programs](https://github.com/UbiquitousLearning/mllm/examples), including the implementation of llama, clip, fuyu, vit, imagebind, and more using the mllm framework. 

In addition, mllm also offers [an example app](https://github.com/UbiquitousLearning/mllm/android) for Android devices, where you can upload models to your phone via adb to experience the effects of different models' inference on mllm.
<style>
{`.demo-video td{
    width: 30%;
    text-align: center;
}`}
</style>
<table class="demo-video">
    <tr>
        <td>demo of UI screen understanding</td>
        <td>demo of image understanding</td>
        <td>demo of LLM chatting</td>
    </tr>
    <tr>
        <td> <video loop controls="controls" muted auto-play src="/demo1.mp4"/> </td>
        <td> <video loop controls="controls" muted auto-play  src="/demo2.mp4"/> </td>
        <td>  <video loop controls="controls" muted auto-play  src="/demo3.mp4"/> </td>
    </tr>
</table>
