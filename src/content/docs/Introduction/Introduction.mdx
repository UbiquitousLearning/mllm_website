---
title: Introduction
description: Introduction to mllm
prev: false
sidebar:
    order: 1
---

**mllm** is a fast multimodal LLM inference engine for mobile and edge devices, mainly supporting CPU inference on Android devices, and also supporting accelerating inference through GPU and NPU methods. 

Currently, mllm supports the inference of large models like <ins>llama</ins>, <ins>fuyu</ins>, <ins>vit</ins>, <ins>imagebind</ins>, <ins>clip</ins>, etc.

## Key Features


## Give it a try
mllm provides a series of [example programs](https://github.com/UbiquitousLearning/mllm/examples), including the implementation of llama, clip, fuyu, vit, imagebind, and more using the mllm framework. 

In addition, mllm also offers [an example app](https://github.com/UbiquitousLearning/mllm/android) for Android devices, where you can upload models to your phone via adb to experience the effects of different models' inference on mllm.

<table>
    <tr>
        <td>Demo1</td>
        <td>Demo2</td>
        <td>Demo3</td>
    </tr>
    <tr>
        <td> <video loop controls="controls" muted auto-play src="/demo1.mp4"/> </td>
        <td> <video loop controls="controls" muted auto-play  src="/demo2.mp4"/> </td>
        <td>  <video loop controls="controls" muted auto-play  src="/demo3.mp4"/> </td>
    </tr>
</table>