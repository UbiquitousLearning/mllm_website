---
title: 添加新模型(旧)
sidebar:
 order: 4
---


:::tip
在添加新模型前，请参阅[模型列表文档](/zh/roadmap/roadmap/#更多模型)，避免不必要的重复。
:::
添加模型包含如下步骤：
1. （可选）[增加新的Tokenizer](#增加新的Tokenizer)
1. （可选）[增加新的PreProcessor](#增加新的PreProcessor)
2. [搭建模型结构](#搭建模型结构)
2. [添加PostProcessing](#添加PostProcessing)
3. [构建处理流程](#构建处理流程)

##  增加新的Tokenizer

需要查阅[tokenizer目录](https://github.com/UbiquitousLearning/mllm/tree/main/src/tokenizers), 检查您想要添加的模型需要的tokenizer是否已经支持，如果已经支持，请跳转至下一章节。

1. 在[tokenizer目录](https://github.com/UbiquitousLearning/mllm/tree/main/src/tokenizers)下添加XXX/XXXTokenizer.hpp、XXX/XXXTokenizer.cpp。
2. 类声明。 继承基类Tokenizer，主要实现tokenize()函数。

示例如下：
```cpp
#ifndef MLLM_XXX_HPP
#define MLLM_XXX_HPP
#include "tokenizers/Tokenizer.hpp"
namespace mllm {
class XXXTokenizer final : public Tokenizer {
    ... ...,

public:
    vector<std::string> XXX(const std::string &token);
    void tokenize(const std::string &text, std::vector<token_id_t> &tokens, bool bos) override;
};
} // namespace mllm

#endif // MLLM_XXX_HPP
```

##  增加新的PreProcessor

需要查阅[processor目录](https://github.com/UbiquitousLearning/mllm/tree/main/src/processor), 检查您想要添加的模型需要的PreProcessor需要的功能是否已经支持，如果已经支持，请跳转至下一章节。尽量不要实现新的PreProcessor。

1. 在[processor目录](https://github.com/UbiquitousLearning/mllm/tree/main/src/processor)下添加XXXPreProcess.hpp、XXXPreProcess.cpp。
2. 类声明。 继承基类PreProcessor，主要重写Process()函数。

示例如下：
```cpp
#ifndef MLLM_XXX_HPP
#define MLLM_XXX_HPP
#include "PreProcess.hpp"
namespace mllm {
class XXXProcessor:public mllm::PreProcessor {
public:
    explicit XXXProcessor(mllm::Tokenizer *tokenizer, int height, int width, bool do_pad, bool do_resize, bool do_normalize, bool do_rescale, std::vector<float> mean) :
        PreProcessor(tokenizer, height, width, do_pad, do_resize, do_normalize, do_rescale, std::move(mean), std::move(std)) {
    }
    void Process(const std::string &text) override;
    void PreProcessImages(const std::vector<std::string> &images_path) override;

};
} // namespace mllm

#endif // MLLM_XXX_HPP
```

## 搭建模型结构

**在[example目录](https://github.com/UbiquitousLearning/mllm/tree/main/examples)下新建main_XXXModel.cpp文件，并在[CMakeList.txt](https://github.com/UbiquitousLearning/mllm/blob/main/CMakeLists.txt)中构建之。**

请参考[example](https://github.com/UbiquitousLearning/mllm/tree/main/examples)和[Op列表文档]()，搭建模型结构。

实例1： 构建CLIP的模型：
```cpp
... ...

void CLIP(Context* c) {
    auto *i = _Input(c, {}, "input_ids");
    i = transformer(c, i);
    auto *p = _Input(c, {}, "input_imgs");
    p = vit(c, p);
    i = _Linear( {i}, 512, 512, false, "text_projection");
    i = *i/i->norm(2);
    p = _Linear( {p}, 768, 512, false, "visual_projection");
    p = *p/p->norm(2);
    auto *o = _Matmul( {i, p}, false, true, "matmul");
    o = _Scale( {o}, 100.0, 0.0F, false, "scale");
}
... ...
```

实例2： 构建Attention结构：
```cpp
... ...

NetTensor *Attention( NetTensor * x, int embedding_size, int hidden_size, int head_size, string name){
    auto *q =_Linear({x}, embedding_size, hidden_size * head_size, false, name + ".wq");
    auto *k =_Linear({x}, embedding_size, hidden_size * head_size, false, name + ".wk");
    auto *v =_Linear({x}, embedding_size, hidden_size * head_size, false, name + ".wv");
    q = q->view(-1, head_size, -1, hidden_size);
    k = k->view(-1, head_size, -1, hidden_size);
    v = v->view(-1, head_size, -1, hidden_size);
    q = _RoPE( {q}, 2, name + ".q_rope");
    k = _RoPE( {k}, 2, name + ".k_rope");
    k = _KVCache( {k},  name + ".k_cache");
    v = _KVCache( {v}, name + ".v_cache");
    auto *qk = _Matmul( {q, k}, false, true, name + ".qk");
    qk = *qk/std::sqrt(hidden_size);
    qk = _Causalmask( {qk}, name + ".mask");
    qk = _Softmax( {qk}, DIMENSION, name + ".softmax");
    auto *o = _Matmul( {qk, v}, false, false, name + ".qkv");
    o = o->view(-1, 1, -1, hidden_size * head_size);
    o = _Linear( {o}, hidden_size * head_size, embedding_size, false, name + ".wo");
    return o;
}
... ...
```
注意： _Op中的的name参数，用于在模型中标识Op，不可重复，且需与模式未转换的pytorch/safetensor模型的对应名称相同。

例如，您可以通过如下方式查看pth模型中Op的名称：
```python
import torch
model_path = "/path/to/model.pth"
model = torch.load(model_path)
for key in model:
    print(f"{key}  Shape: {model[key].shape}")

```
输出如下：
```
tok_embeddings.weight  Shape: torch.Size([32000, 4096])
norm.weight  Shape: torch.Size([4096])
output.weight  Shape: torch.Size([32000, 4096])
layers.0.attention.wq.weight  Shape: torch.Size([4096, 4096])
layers.0.attention.wk.weight  Shape: torch.Size([4096, 4096])
layers.0.attention.wv.weight  Shape: torch.Size([4096, 4096])
layers.0.attention.wo.weight  Shape: torch.Size([4096, 4096])
layers.0.feed_forward.w1.weight  Shape: torch.Size([11008, 4096])
... ...
```
上述代码将打印出模型中所有Op的“key”值中“.weight”前的部分需与mllm中的name参数相同。
例如： `"layers.0.attention.wq.weight"`对应的mllm中的name参数应为`"layers.0.attention.wq"`。

## 添加PostProcessing
在main_XXXModel.cpp中添加PostProcessing函数。
此函数用于处理模型的输出。
例如：

```cpp
vector<float> postProcessing(shared_ptr<Tensor> result){
    vector<float> scores;
    for (int i = 0; i < result->batch(); ++i) {
        auto value = result->dataAt<float>(i, 0, 0, 0);
        scores.push_back(value);
    }
    auto token_idx =  softmax(scores);
    return token_idx;
}
```

## 构建处理流程

在main_XXXModel.cpp的main()中按照如下流程进行处理。

1. 新建Context，构建模型。
```cpp
std::unique_ptr<Context> c_ptr(new Context());
auto *c = c_ptr.get();
CLIP(c);
```

2. 选择Backend，从Context构建Net。
```cpp
BackendConfig bn;
Net net(bn);
net.convert(c->sub_param_);
```

3. 构建Executor，加载模型权重。
```cpp
string model_path = "model.ckpt.mllm";
ParamLoader param_loader(model_path);
Executor ex(&param_loader);
ex.setup(&net);
```

4. Tokenizer根据词表对输入sequences处理。
```cpp
string vocab_path = "vocab.mllm";
auto tokenizer = new BPETokenizer(vocab_path);
... ...

vector<string> in_strs = {"a photo of a cat"};
auto tokens_ids = vector<vector<token_id_t>>();
for (auto in_str : in_strs) {
    vector<mllm::token_id_t> tokens_id={};
    tokenizer->tokenize(in_str, tokens_id, true);
        tokens_ids.push_back(tokens_id);
}
shared_ptr<Tensor> input_text = std::make_shared<Tensor>();
BPETokenizer::tokens2Tensor(&net, tokens_ids, input_text);
```

5. (可选)Processor处理图片等其他模态输入。
```cpp
shared_ptr<Tensor> input_img = std::make_shared<Tensor>();
auto* processor = new ClipProcessor(tokenizer);
processor->PreProcessImages({"cat.jpg"});
auto images = processor->pixel_values_[0];
img2Tensor(input_img, net, images);
```

6. 进行模型推理。注意输入顺序需要与[搭建模型结构](#搭建模型结构)时的`_Input`顺序相同！！
```cpp
ex.run(&net, {input_text, input_img});
auto result = ex.result();

7. 进行postProcessing。
```cpp
auto probs = postProcessing(result[0]);
```